{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-26T09:09:57.279861300Z",
     "start_time": "2023-09-26T09:09:55.934223300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#path to the directory containing json files\n",
    "datasetpath = 'data/massive_dataset/data/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T09:09:59.890148900Z",
     "start_time": "2023-09-26T09:09:59.876001500Z"
    }
   },
   "id": "6c25efb187b5ade6"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from all JSONL files has been loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#library to resolve paths in the working directories\n",
    "\n",
    "# List all JSONL files in the directory\n",
    "jsonl_files = [f for f in os.listdir(datasetpath) if f.endswith('.jsonl')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each JSONL file and read its contents into a DataFrame\n",
    "for jsonl_file in jsonl_files:\n",
    "    try:\n",
    "        # Construct the full file path\n",
    "        jsonl_file_path = os.path.join(datasetpath, jsonl_file)\n",
    "\n",
    "        # Read the JSONL file into a DataFrame, assuming one JSON object per line\n",
    "        df = pd.read_json(jsonl_file_path, lines=True)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {jsonl_file}: {e}\")\n",
    "\n",
    "# Check if any DataFrames were created\n",
    "if dataframes:\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Now, combined_dataframe contains data from all JSONL files as a single DataFrame\n",
    "    print(\"Data from all JSONL files has been loaded.\")\n",
    "else:\n",
    "    print(\"No valid data found in the JSONL files.\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T09:10:39.853672300Z",
     "start_time": "2023-09-26T09:10:02.116824100Z"
    }
   },
   "id": "5950bcc038854f87"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f0e3cec0386a746f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                  id      worker_id\ncount  859092.000000  859092.000000\nmean     8564.718661      24.415974\nstd      4955.577530      53.467818\nmin         0.000000       0.000000\n25%      4273.000000       6.000000\n50%      8534.000000      15.000000\n75%     12837.000000      28.000000\nmax     17180.000000     690.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>worker_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>859092.000000</td>\n      <td>859092.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>8564.718661</td>\n      <td>24.415974</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4955.577530</td>\n      <td>53.467818</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4273.000000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>8534.000000</td>\n      <td>15.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>12837.000000</td>\n      <td>28.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>17180.000000</td>\n      <td>690.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataframe.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T09:10:40.000183400Z",
     "start_time": "2023-09-26T09:10:39.875111900Z"
    }
   },
   "id": "459bbe2555836f8a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for language af-ZA has been saved to data/generated_excel_files_all/en-af-ZA.xlsx\n",
      "Data for language am-ET has been saved to data/generated_excel_files_all/en-am-ET.xlsx\n",
      "Data for language ar-SA has been saved to data/generated_excel_files_all/en-ar-SA.xlsx\n",
      "Data for language az-AZ has been saved to data/generated_excel_files_all/en-az-AZ.xlsx\n",
      "Data for language bn-BD has been saved to data/generated_excel_files_all/en-bn-BD.xlsx\n",
      "Data for language ca-ES has been saved to data/generated_excel_files_all/en-ca-ES.xlsx\n",
      "Data for language cy-GB has been saved to data/generated_excel_files_all/en-cy-GB.xlsx\n",
      "Data for language da-DK has been saved to data/generated_excel_files_all/en-da-DK.xlsx\n",
      "Data for language de-DE has been saved to data/generated_excel_files_all/en-de-DE.xlsx\n",
      "Data for language el-GR has been saved to data/generated_excel_files_all/en-el-GR.xlsx\n",
      "Data for language en-US has been saved to data/generated_excel_files_all/en-en-US.xlsx\n",
      "Data for language es-ES has been saved to data/generated_excel_files_all/en-es-ES.xlsx\n",
      "Data for language fa-IR has been saved to data/generated_excel_files_all/en-fa-IR.xlsx\n",
      "Data for language fi-FI has been saved to data/generated_excel_files_all/en-fi-FI.xlsx\n",
      "Data for language fr-FR has been saved to data/generated_excel_files_all/en-fr-FR.xlsx\n",
      "Data for language he-IL has been saved to data/generated_excel_files_all/en-he-IL.xlsx\n",
      "Data for language hi-IN has been saved to data/generated_excel_files_all/en-hi-IN.xlsx\n",
      "Data for language hu-HU has been saved to data/generated_excel_files_all/en-hu-HU.xlsx\n",
      "Data for language hy-AM has been saved to data/generated_excel_files_all/en-hy-AM.xlsx\n",
      "Data for language id-ID has been saved to data/generated_excel_files_all/en-id-ID.xlsx\n",
      "Data for language is-IS has been saved to data/generated_excel_files_all/en-is-IS.xlsx\n",
      "Data for language it-IT has been saved to data/generated_excel_files_all/en-it-IT.xlsx\n",
      "Data for language ja-JP has been saved to data/generated_excel_files_all/en-ja-JP.xlsx\n",
      "Data for language jv-ID has been saved to data/generated_excel_files_all/en-jv-ID.xlsx\n",
      "Data for language ka-GE has been saved to data/generated_excel_files_all/en-ka-GE.xlsx\n",
      "Data for language km-KH has been saved to data/generated_excel_files_all/en-km-KH.xlsx\n",
      "Data for language kn-IN has been saved to data/generated_excel_files_all/en-kn-IN.xlsx\n",
      "Data for language ko-KR has been saved to data/generated_excel_files_all/en-ko-KR.xlsx\n",
      "Data for language lv-LV has been saved to data/generated_excel_files_all/en-lv-LV.xlsx\n",
      "Data for language ml-IN has been saved to data/generated_excel_files_all/en-ml-IN.xlsx\n",
      "Data for language mn-MN has been saved to data/generated_excel_files_all/en-mn-MN.xlsx\n",
      "Data for language ms-MY has been saved to data/generated_excel_files_all/en-ms-MY.xlsx\n",
      "Data for language my-MM has been saved to data/generated_excel_files_all/en-my-MM.xlsx\n",
      "Data for language nb-NO has been saved to data/generated_excel_files_all/en-nb-NO.xlsx\n",
      "Data for language nl-NL has been saved to data/generated_excel_files_all/en-nl-NL.xlsx\n",
      "Data for language pl-PL has been saved to data/generated_excel_files_all/en-pl-PL.xlsx\n",
      "Data for language pt-PT has been saved to data/generated_excel_files_all/en-pt-PT.xlsx\n",
      "Data for language ro-RO has been saved to data/generated_excel_files_all/en-ro-RO.xlsx\n",
      "Data for language ru-RU has been saved to data/generated_excel_files_all/en-ru-RU.xlsx\n",
      "Data for language sl-SL has been saved to data/generated_excel_files_all/en-sl-SL.xlsx\n",
      "Data for language sq-AL has been saved to data/generated_excel_files_all/en-sq-AL.xlsx\n",
      "Data for language sv-SE has been saved to data/generated_excel_files_all/en-sv-SE.xlsx\n",
      "Data for language sw-KE has been saved to data/generated_excel_files_all/en-sw-KE.xlsx\n",
      "Data for language ta-IN has been saved to data/generated_excel_files_all/en-ta-IN.xlsx\n",
      "Data for language te-IN has been saved to data/generated_excel_files_all/en-te-IN.xlsx\n",
      "Data for language th-TH has been saved to data/generated_excel_files_all/en-th-TH.xlsx\n",
      "Data for language tl-PH has been saved to data/generated_excel_files_all/en-tl-PH.xlsx\n",
      "Data for language tr-TR has been saved to data/generated_excel_files_all/en-tr-TR.xlsx\n",
      "Data for language ur-PK has been saved to data/generated_excel_files_all/en-ur-PK.xlsx\n",
      "Data for language vi-VN has been saved to data/generated_excel_files_all/en-vi-VN.xlsx\n",
      "Data for language zh-CN has been saved to data/generated_excel_files_all/en-zh-CN.xlsx\n",
      "Data for language zh-TW has been saved to data/generated_excel_files_all/en-zh-TW.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Get the unique languages in the DataFrame\n",
    "unique_languages = combined_dataframe['locale'].unique()\n",
    "\n",
    "# Specify the directory for generated Excel files\n",
    "excel_output_directory = 'data/generated_excel_files_all/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(excel_output_directory, exist_ok=True)\n",
    "    \n",
    "# Filter and save data for each language in separate Excel files\n",
    "for lang in unique_languages:\n",
    "    # Filter data for the current language\n",
    "    lang_data = combined_dataframe[combined_dataframe['locale'] == lang]\n",
    "    \n",
    "   # Create a unique Excel filename for each locale, e.g., en-xx.xlsx\n",
    "    excel_filename = os.path.join(excel_output_directory, f'en-{lang}.xlsx')\n",
    "    \n",
    "    # Save the filtered data to the Excel file\n",
    "    lang_data.to_excel(excel_filename, index=False)\n",
    "    print(f'Data for language {lang} has been saved to {excel_filename}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T09:14:22.606631700Z",
     "start_time": "2023-09-26T09:10:44.207658200Z"
    }
   },
   "id": "7f1bdd783a1a4ee0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Languages: ['en-US', 'sw-KE', 'de-DE']\n",
      "Unique Languages: ['af-ZA' 'am-ET' 'ar-SA' 'az-AZ' 'bn-BD' 'ca-ES' 'cy-GB' 'da-DK' 'de-DE'\n",
      " 'el-GR' 'en-US' 'es-ES' 'fa-IR' 'fi-FI' 'fr-FR' 'he-IL' 'hi-IN' 'hu-HU'\n",
      " 'hy-AM' 'id-ID' 'is-IS' 'it-IT' 'ja-JP' 'jv-ID' 'ka-GE' 'km-KH' 'kn-IN'\n",
      " 'ko-KR' 'lv-LV' 'ml-IN' 'mn-MN' 'ms-MY' 'my-MM' 'nb-NO' 'nl-NL' 'pl-PL'\n",
      " 'pt-PT' 'ro-RO' 'ru-RU' 'sl-SL' 'sq-AL' 'sv-SE' 'sw-KE' 'ta-IN' 'te-IN'\n",
      " 'th-TH' 'tl-PH' 'tr-TR' 'ur-PK' 'vi-VN' 'zh-CN' 'zh-TW']\n",
      "dooooooooneeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "emptyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "Length of shuffled data for Locale de-DE: 16521\n",
      "Length of train data for Locale de-DE: 9912\n",
      "emptyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "Length of shuffled data for Locale en-US: 16521\n",
      "Length of train data for Locale en-US: 9912\n",
      "emptyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "Length of shuffled data for Locale sw-KE: 16521\n",
      "Length of train data for Locale sw-KE: 9912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the selected locales\n",
    "selected_languages = [\"en-US\", \"sw-KE\", \"de-DE\"]\n",
    "print(\"Selected Languages:\", selected_languages)\n",
    "print(\"Unique Languages:\", unique_languages)\n",
    "\n",
    "\n",
    "# Initialize dictionaries to store train, validation, and test datasets for each locale\n",
    "train_datasets = {}\n",
    "valid_datasets = {}\n",
    "test_datasets = {}\n",
    "\n",
    "# Extract, shuffle, and partition data for each locale\n",
    "\n",
    "for lang in unique_languages:\n",
    "    # Filter data for the current language\n",
    "    lang_data = combined_dataframe[combined_dataframe['locale'] == lang]\n",
    "    \n",
    "    # Shuffle data only for selected locales (\"en,\" \"sw,\" and \"de\")\n",
    "    if lang in selected_languages:\n",
    "        shuffled_data = lang_data.sample(frac=1)  # Shuffle the data\n",
    "        if len(shuffled_data) > 10:\n",
    "            print('emptyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy')\n",
    "            \n",
    "        print(f\"Length of shuffled data for Locale {lang}: {len(shuffled_data)}\")\n",
    "        \n",
    "        # Split the shuffled data into train, validation, and test datasets\n",
    "        train_data, valid_test_data = train_test_split(shuffled_data, test_size=0.4, random_state=42)\n",
    "        valid_data, test_data = train_test_split(valid_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Store the datasets in dictionaries\n",
    "        train_datasets[lang] = train_data\n",
    "        valid_datasets[lang] = valid_data\n",
    "        test_datasets[lang] = test_data\n",
    "        print(f\"Length of train data for Locale {lang}: {len(train_datasets[lang])}\")\n",
    "        \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T09:14:30.423112800Z",
     "start_time": "2023-09-26T09:14:22.603630900Z"
    }
   },
   "id": "6722ec2eb45a2846"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the directory where you want to save the JSONL files\n",
    "output_directory = 'data/generated_jsonl/'  # Replace with the actual path\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over the selected locales\n",
    "for lang in selected_languages:\n",
    "    # Define the filenames for the JSONL files\n",
    "    train_filename = os.path.join(output_directory, f'{lang}_train.jsonl')\n",
    "    valid_filename = os.path.join(output_directory, f'{lang}_valid.jsonl')\n",
    "    test_filename = os.path.join(output_directory, f'{lang}_test.jsonl')\n",
    "\n",
    "    # Write the train data to the JSONL file\n",
    "    with open(train_filename, 'w', encoding='utf-8') as train_file:\n",
    "        for index, row in train_datasets[lang].iterrows():\n",
    "            json.dump(row.to_dict(), train_file, ensure_ascii=False)\n",
    "            train_file.write('\\n')\n",
    "\n",
    "    # Write the validation data to the JSONL file\n",
    "    with open(valid_filename, 'w', encoding='utf-8') as valid_file:\n",
    "        for index, row in valid_datasets[lang].iterrows():\n",
    "            json.dump(row.to_dict(), valid_file, ensure_ascii=False)\n",
    "            valid_file.write('\\n')\n",
    "\n",
    "    # Write the test data to the JSONL file\n",
    "    with open(test_filename, 'w', encoding='utf-8') as test_file:\n",
    "        for index, row in test_datasets[lang].iterrows():\n",
    "            json.dump(row.to_dict(), test_file, ensure_ascii=False)\n",
    "            test_file.write('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T19:40:58.352975Z",
     "start_time": "2023-09-25T19:40:47.212818600Z"
    }
   },
   "id": "c650f80b8d594a98"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations have been saved to data/translations_jsonl/translations.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Initialize an empty list to store the translations\n",
    "translations = []\n",
    "\n",
    "# Specify the output JSON file name and directory\n",
    "output_directory = 'data/translations_jsonl/'  # Specify the directory\n",
    "output_json_file = os.path.join(output_directory, 'translations.json')\n",
    "\n",
    "# Iterate over the selected locales\n",
    "for lang in selected_languages:\n",
    "    # Filter the training data for the current locale\n",
    "    train_data = train_datasets[lang]\n",
    "    \n",
    "    # Extract translations from English (en) to the current locale (xx)\n",
    "    translations_lang = {\n",
    "        'from_language': 'en',\n",
    "        'to_language': lang,\n",
    "        'translations': train_data[['id', 'utt']].to_dict(orient='records')\n",
    "    }\n",
    "    \n",
    "    # Append the translations to the list\n",
    "    translations.append(translations_lang)\n",
    "\n",
    "# Create a dictionary to store all translations\n",
    "all_translations = {\n",
    "    'translations': translations\n",
    "}\n",
    "\n",
    "# Write the translations to the output JSON file with pretty formatting\n",
    "with open(output_json_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(all_translations, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Translations have been saved to {output_json_file}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T09:14:30.687112Z",
     "start_time": "2023-09-26T09:14:30.427123900Z"
    }
   },
   "id": "64de97b58a117135"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "387d2efa2444b5bd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
